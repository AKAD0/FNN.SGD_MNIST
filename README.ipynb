{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents:\n",
    "1. Describtion\n",
    "2. Problem\n",
    "3. Method\n",
    "    * 3.1. Model\n",
    "    * 3.2. Computation Graph\n",
    "    * 3.3. Back Propagation\n",
    "    * 3.4. Stochastic Gradient Descent\n",
    "4. Code\n",
    "5. Results\n",
    "\n",
    "# 1. Describtion\n",
    "Optimization is exactly the reason why models learn (approximate to a desired function) and virtually all the Deep Learning algorithms are optimized by so-called Stochastic Gradient Descent method (or some variations of it).\n",
    "\n",
    "This project presents the mathematical apparatus of gradient descent on the example of the Neural Network of FNN architecture designed for MNIST classification problem. It offers only mathematical approach whereas provided program is a python pseudo-code of described functions.\n",
    "\n",
    "# 2. Problem\n",
    "Apparatus of stochastic gradient descent on the example of MNIST problem.\n",
    "\n",
    "# 3. Method\n",
    "Project describes the apparatus in 4 parts: Model, Computation Graph, Back Propagation and Gradient Descent. Describtion of every part is provided below.\n",
    "\n",
    "## 3.1. Model\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/AKAD0/FNN.SGD_MNIST/blob/master/Fig1.png\">\n",
    "</p>\n",
    "\n",
    "$$\n",
    "\\text{Fig.1: Topology of the architecture}\n",
    "$$\n",
    "\n",
    "Topology consists of 2 hidden layers 10 blocks each, 1 output layer of 10 blocks and 1 input layer of 784 blocks (28*28 pixels of an image to process).\n",
    "\n",
    "### 3.2. Computation Graph:\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/AKAD0/FNN.SGD_MNIST/blob/master/Fig2_v2.png\">\n",
    "</p>\n",
    "\n",
    "$$\n",
    "\\text{Fig.2: Computation graph}\n",
    "$$\n",
    "\n",
    "$w$ and $b$ parameters are vectors of weights and biases respectively. $y$ is a label mark for $\\hat{y}=a^{(y)}$ to compare to.\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "C=MSE=\\frac{1}{10}\\sum_{i=1}^{10}(y_i-a_i^{(y)})^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{where}~\n",
    "&C=MSE-\\text{Mean Square Error cost function} \\\\\n",
    "&y-\\text{true label} \\\\\n",
    "&a^{(y)}-\\text{result of the activation function} \\\\\n",
    "&i=10-\\text{number of predicting classes}~f^{(1)} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "a^{(y)}=softmax(z^{(y)})= \\frac{e^{z_i^{(y)}}}{\\sum\\limits_{j=1}^{10} e^{z_j^{(y)}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{where}~\n",
    "&a^{(y)}-\\text{activation function of output layer (y)} \\\\\n",
    "&z^{(y)}-\\text{input function (affine transformation) of output layer (y)} \\\\\n",
    "&j=10-\\text{number of predicting classes}~f^{(1)} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "z^{(y)}=\\sum_{i=1}^{10}(w_i^{(y)T}a_i^{(2)})+b^{(y)T}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{where}~\n",
    "&z^{(y)}-\\text{input function (affine transformation) of output layer (y)} \\\\\n",
    "&w^{(y)}-\\text{weights vector of output layer (y)} \\\\\n",
    "&b^{(y)}-\\text{biases vector of output layer (y)} \\\\\n",
    "&a^{(2)}-\\text{activation function of the 2nd hidden layer} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "a^{(2)}=ReLU=max\\lbrace 0,z^{(2)}\\rbrace=\n",
    "\\begin{Bmatrix}\n",
    "  z^{(2)},~z^{(2)}>0 \\\\\n",
    "  0,~z^{(2)}≤0\n",
    "\\end{Bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{where}~\n",
    "&a^{(2)}-\\text{activation function of the 2nd hidden layer} \\\\\n",
    "&z^{(2)}-\\text{input function (affine transformation) of 2nd hidden layer} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "z^{(2)}=\\sum_{i=1}^{10}(w_i^{(2)T}a_i^{(1)})+b^{(2)T}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{where}~\n",
    "&z^{(2)}-\\text{input function (affine transformation) of 2nd hidden layer} \\\\\n",
    "&w^{(2)}-\\text{weights vector of 2nd hidden layer} \\\\\n",
    "&b^{(2)T}-\\text{biases vector of 2nd hidden layer} \\\\\n",
    "&a^{(1)}-\\text{activation function of the 1st hidden layer} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "a^{(1)}=ReLU=max\\lbrace 0,z^{(1)}\\rbrace=\n",
    "\\begin{Bmatrix}\n",
    "  z^{(1)},~z^{(1)}>0 \\\\\n",
    "  0,~z^{(1)}≤0\n",
    "\\end{Bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{where}~\n",
    "&a^{(1)}-\\text{activation function of the 1st hidden layer} \\\\\n",
    "&z^{(1)}-\\text{input function (affine transformation) of 1st hidden layer} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "z^{(1)}=\\sum_{i=1}^{784}(w_i^{(1)T}x_i)+b^{(1)T}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{where}~\n",
    "&z^{(2)}-\\text{input function (affine transformation) of 2nd hidden layer} \\\\\n",
    "&w^{(2)}-\\text{weights vector of 2nd hidden layer} \\\\\n",
    "&b^{(2)T}-\\text{biases vector of 2nd hidden layer} \\\\\n",
    "&a^{(1)}-\\text{activation function of the 1st hidden layer} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 3.3. Back Propagation:\n",
    "Back Propagation is a process of finding derivatives of the Cost function with respect to the Theta parameters - weights and biases.\n",
    "It is done in two steps:\n",
    "1) Find all the derivatives in the computation graph for further use in the chain rule.\n",
    "2) Find said 'Theta' derivatives via chain rule.\n",
    "\n",
    "#### First step. Finding every derivative in the computation graph.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{1)}~\\frac{dC}{da^{(y)}}~\n",
    "&=\\bigg| \\frac{d\\frac{1}{10}((y_1-a_1)^2+(y_2-a_2)^2+...+(y_10-a_10)^2)}{da_1} \\\\\n",
    "&= -\\frac{1}{10}2(y_1-a_1)=\\frac{-2(y_1-a_1)}{10}=\\frac{-(y_1-a_1)}{5} \\bigg|\\\\\n",
    "&=\\left[ \\frac{-(y_1-a_1)}{5},~...~,\\frac{-(y_10-a_10)}{5} \\right]^T\\\\\n",
    "&=\\frac{-(y-a^{(y)})}{5} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{2)}\\frac{da^{(y)}}{dz^{(y)}}=DZ^{(y)}_{R^{10}}\n",
    "&=\\left| \\text{Quotient rule} \\right| \\\\\n",
    "&=\n",
    "\\left[ \n",
    "\\begin{matrix} \n",
    "a^{(y)}_1(1-a^{(y)}_1), & a^{(y)}_2a^{(y)}_1,     & ~...~,  & a^{(y)}\\_{10}a^{(y)}_1 \\\\\n",
    "a^{(y)}_1a^{(y)}_2,     & a^{(y)}_2(1-a^{(y)}_2), & ~...~,  & a^{(y)}\\_{10}a^{(y)}_2 \\\\\n",
    "~...~,                  & ~...~,                  & ~...~,  & ~...~ \\\\\n",
    "a^{(y)}_1a^{(y)}\\_{10},  & a^{(y)}_2a^{(y)}\\_{10},  & ~...~,  & a^{(y)}\\_{10}(1-a^{(y)}\\_{10})\n",
    "\\end{matrix}\n",
    "\\right] \\\\\n",
    "&=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\sum\\limits\\_{k=1}\\^{10} (a^{(y)}_ka^{(y)}_1)-a^{(y)}_1a^{(y)}_1+a^{(y)}_1(1-a^{(y)}_1) \\\\\n",
    "~...~ \\\\\n",
    "\\sum\\limits\\_{k=1}\\^{10} (a^{(y)}_ka^{(y)}\\_{10})-a^{(y)}\\_{10}a^{(y)}\\_{10}+a^{(y)}\\_{10}(1-a^{(y)}\\_{10})\n",
    "\\end{matrix}\n",
    "\\right] \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\text{3)}~\\frac{dz^{(y)}}{da^{(2)}}=w^{(y)}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\text{4)}~\\frac{da^{(2)}}{dz^{(2)}}= \n",
    "\\begin{Bmatrix}\n",
    "  1,~z^{(2)}>0 \\\\\n",
    "  0,~z^{(2)}≤0\n",
    "\\end{Bmatrix}\n",
    "\\implies\n",
    "𝟙_{R>0}(z^{(2)})\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\text{5)}~\\frac{dz^{(2)}}{da^{(1)}}=w^{(2)}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\text{6)}~\\frac{da^{(1)}}{dz^{(1)}}= \n",
    "\\begin{Bmatrix}\n",
    "  1,~z^{(1)}>0 \\\\\n",
    "  0,~z^{(1)}≤0\n",
    "\\end{Bmatrix}\n",
    "\\implies\n",
    "𝟙_{R>0}(z^{(1)})\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\text{7)}~\\frac{dz^{(y)}}{dw^{(y)}}=a^{(2)}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\text{8)}~\\frac{dz^{(y)}}{db^{(y)}}=1\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\text{9)}~\\frac{dz^{(2)}}{dw^{(2)}}=a^{(1)}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\text{10)}~\\frac{dz^{(2)}}{db^{(2)}}=1\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\text{11)}~\\frac{dz^{(1)}}{dw^{(1)}}=x\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\text{12)}~\\frac{dz^{(1)}}{db^{(1)}}=1\n",
    "$$\n",
    "\n",
    "#### Second step. Finding 'Theta' derivatives via Chain rule.\n",
    "$$\n",
    "\\text{1)}~\\frac{dC}{dw^{(y)}} = \\frac{dC}{da^{(y)}} \\frac{da^{(y)}}{dz^{(y)}} \\frac{dz^{(y)}}{dw^{(y)}}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\text{2)}~\\frac{dC}{db^{(y)}} = \\frac{dC}{da^{(y)}} \\frac{da^{(y)}}{dz^{(y)}} \\frac{dz^{(y)}}{db^{(y)}}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\text{3)}~\\frac{dC}{dw^{(2)}} = \\frac{dC}{da^{(y)}} \\frac{da^{(y)}}{dz^{(y)}} \\frac{dz^{(y)}}{da^{(2)}} \\frac{da^{(2)}}{dz^{(2)}} \\frac{dz^{(2)}}{dw^{(2)}}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\text{4)}~\\frac{dC}{db^{(2)}} = \\frac{dC}{da^{(y)}} \\frac{da^{(y)}}{dz^{(y)}} \\frac{dz^{(y)}}{da^{(2)}} \\frac{da^{(2)}}{dz^{(2)}} \\frac{dz^{(2)}}{db^{(2)}}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\text{5)}~\\frac{dC}{dw^{(1)}} = \\frac{dC}{da^{(y)}} \\frac{da^{(y)}}{dz^{(y)}} \\frac{dz^{(y)}}{da^{(2)}} \\frac{da^{(2)}}{dz^{(2)}} \\frac{dz^{(2)}}{da^{(1)}} \\frac{da^{(1)}}{dz^{(1)}} \\frac{dz^{(1)}}{dw^{(1)}}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\text{6)}~\\frac{dC}{db^{(1)}} = \\frac{dC}{da^{(y)}} \\frac{da^{(y)}}{dz^{(y)}} \\frac{dz^{(y)}}{da^{(2)}} \\frac{da^{(2)}}{dz^{(2)}} \\frac{dz^{(2)}}{da^{(1)}} \\frac{da^{(1)}}{dz^{(1)}} \\frac{dz^{(1)}}{db^{(1)}}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&S_1 = \\frac{dC}{da^{(y)}} \\frac{da^{(y)}}{dz^{(y)}} = \\frac{-(y-a^{(y)})}{5}DZ^{(y)} \\\\\n",
    "&S_2 = S_1 \\frac{dz^{(y)}}{da^{(2)}} \\frac{da^{(2)}}{dz^{(2)}} = S_1 w^{(y)} 𝟙_{R>0}(z^{2}) \\\\\n",
    "&S_3 = S_1 S_2 \\frac{dz^{(2)}}{da^{(1)}} \\frac{da^{(1)}}{dz^{(1)}} = S_1 S_2 w^{(2)} 𝟙_{R>0}(z^{1}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\nabla _{w{(y)}} C = \\frac{dC}{dw^{(y)}} = S_1a^{(2)} \\\\\n",
    "&\\nabla _{b{(y)}} C = \\frac{dC}{db^{(y)}} = S_1 \\\\\n",
    "&\\nabla _{w{(2)}} C = \\frac{dC}{dw^{(2)}} = S_1 S_2 a^{(1)} \\\\\n",
    "&\\nabla _{b{(2)}} C = \\frac{dC}{db^{(2)}} = S_1 S_2 \\\\\n",
    "&\\nabla _{w{(1)}} C = \\frac{dC}{dw^{(1)}} = S_1 S_2 S_3 x \\\\\n",
    "&\\nabla _{b{(1)}} C = \\frac{dC}{db^{(1)}} = S_1 S_2 S_3 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 3.4. Stochastic Gradient Descent:\n",
    "Gradient Descent in essence is a process of repeated steps of subtractions from current values of Theta parameters (weights and biases) the mean value of every found gradients for every sample which is multiplied by an epsilon coefficient to set learning 'rate', thus approximating to the local minimum of the Cost function.\n",
    "\n",
    "Stochastic Gradient Descent modifies vanilla approach in order to drastically accelerate computation via dividing the whole dataset into randomly (stochastically) ordered 'mini-batches' and processing of such a single batch constitutes single step. Since mini-batch is way smaller than a whole dataset, it's processing takes less time, however with less precision.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\nabla _{w{(y)}}\\^{\\text{batch}} C = \\frac{1}{K} \\sum\\limits\\_{i=1}\\^{K} \\nabla _{w{(y)}i} C \\\\\n",
    "&\\nabla _{b{(y)}}\\^{\\text{batch}} C = \\frac{1}{K} \\sum\\limits\\_{i=1}\\^{K} \\nabla _{b{(y)}i} C \\\\\n",
    "&\\nabla _{w{(2)}}\\^{\\text{batch}} C = \\frac{1}{K} \\sum\\limits\\_{i=1}\\^{K} \\nabla _{w{(2)}i} C \\\\\n",
    "&\\nabla _{b{(2)}}\\^{\\text{batch}} C = \\frac{1}{K} \\sum\\limits\\_{i=1}\\^{K} \\nabla _{b{(2)}i} C \\\\\n",
    "&\\nabla _{w{(1)}}\\^{\\text{batch}} C = \\frac{1}{K} \\sum\\limits\\_{i=1}\\^{K} \\nabla _{w{(1)}i} C \\\\\n",
    "&\\nabla _{b{(1)}}\\^{\\text{batch}} C = \\frac{1}{K} \\sum\\limits\\_{i=1}\\^{K} \\nabla _{b{(1)}i} C \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where K $-$ number of samples in mini-batch}\n",
    "$$\n",
    "\n",
    "‎<br>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&w^{(y)} = w^{(y)}-\\epsilon\\nabla _{w{(y)}}\\^{\\text{batch}} C \\\\\n",
    "&b^{(y)} = b^{(y)}-\\epsilon\\nabla _{b{(y)}}\\^{\\text{batch}} C \\\\\n",
    "&w^{(2)} = w^{(2)}-\\epsilon\\nabla _{w{(2)}}\\^{\\text{batch}} C \\\\\n",
    "&b^{(2)} = b^{(2)}-\\epsilon\\nabla _{b{(2)}}\\^{\\text{batch}} C \\\\\n",
    "&w^{(1)} = w^{(1)}-\\epsilon\\nabla _{w{(1)}}\\^{\\text{batch}} C \\\\\n",
    "&b^{(1)} = b^{(1)}-\\epsilon\\nabla _{b{(1)}}\\^{\\text{batch}} C \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where $\\epsilon$ $-$ learning rate}\n",
    "$$\n",
    "\n",
    "# 4. Code\n",
    "The listing of the python pseudo-code for described algorithm is provided in the '/FNN.SGD_MNIST.ipynb' file.\n",
    "\n",
    "# 5. Results\n",
    "The provided approach allows a developer to thoroughly understand and implement an algorithm of the FNN architecture for the MNIST classification problem with the optimization of Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "//Latex code for Fig.1\n",
    "\n",
    "\\documentclass{article} \n",
    "\\usepackage{tikz} \n",
    "\\begin{document} \n",
    "\\begin{tikzpicture}[node distance={20mm}, main/.style = {draw, circle}] \n",
    "\\node[main] (1) {$x_{R^{784*1}}$};\n",
    "\\node[main] (2) [right of=1, label=below:] {$h_{R^{10*1}}^{(1)}$};\n",
    "\\node[main] (3) [right of=2, label=below:] {$h_{R^{10*1}}^{(2)}$};\n",
    "\\node[main] (4) [right of=3, label=below:] {$\\hat{y}_{R^{10*1}}$};\n",
    "\\draw[->] (1) -- (2);\n",
    "\\draw[->] (2) -- (3);\n",
    "\\draw[->] (3) -- (4);\n",
    "\\end{tikzpicture} \n",
    "\\end{document}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "//Latex code for Fig.2\n",
    "\n",
    "\\documentclass{article} \n",
    "\\usepackage{tikz} \n",
    "\\begin{document} \n",
    "\\begin{tikzpicture}[node distance={20mm}, main/.style = {draw, circle}] \n",
    "\\node[main] (1) [minimum size=1cm]{$w^{(1)}$};\n",
    "\\node[main] (2) [right of=1, minimum size=1cm] {$x$};\n",
    "\\node[main] (3) [right of=2, minimum size=1cm] {$b^{(1)}$};\n",
    "\\node[main] (4) [below of=2, minimum size=1cm] {$z^{(1)}$};\n",
    "\\node[main] (5) [below of=4, minimum size=1cm] {$a^{(1)}$};\n",
    "\\node[main] (6) [left of=5, minimum size=1cm] {$w^{(2)}$};\n",
    "\\node[main] (7) [right of=5, minimum size=1cm] {$b^{(2)}$};\n",
    "\\node[main] (8) [below of=5, minimum size=1cm] {$z^{(2)}$};\n",
    "\\node[main] (9) [below of=8, minimum size=1cm] {$a^{(2)}$};\n",
    "\\node[main] (10) [left of=9, minimum size=1cm] {$w^{(y)}$};\n",
    "\\node[main] (11) [right of=9, minimum size=1cm] {$b^{(y)}$};\n",
    "\\node[main] (12) [below of=9, minimum size=1cm] {$z^{(y)}$};\n",
    "\\node[main] (13) [below of=12, minimum size=1cm] {$a^{(y)}$};\n",
    "\\node[main] (14) [left of=13, minimum size=1cm] {$y$};\n",
    "\\node[main] (15) [below of=13, minimum size=1cm] {$C$};\n",
    "\\draw[-] (1) -- (4);\n",
    "\\draw[-] (2) -- (4);\n",
    "\\draw[-] (3) -- (4);\n",
    "\\draw[-] (4) -- (5);\n",
    "\\draw[-] (5) -- (8);\n",
    "\\draw[-] (6) -- (8);\n",
    "\\draw[-] (7) -- (8);\n",
    "\\draw[-] (8) -- (9);\n",
    "\\draw[-] (10) -- (12);\n",
    "\\draw[-] (9) -- (12);\n",
    "\\draw[-] (11) -- (12);\n",
    "\\draw[-] (12) -- (13);\n",
    "\\draw[-] (13) -- (15);\n",
    "\\draw[-] (14) -- (15);\n",
    "\\end{tikzpicture} \n",
    "\\end{document}\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
